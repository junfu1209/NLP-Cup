{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7740846,"sourceType":"datasetVersion","datasetId":4524347},{"sourceId":7741042,"sourceType":"datasetVersion","datasetId":4524539},{"sourceId":7893017,"sourceType":"datasetVersion","datasetId":4634330},{"sourceId":7907523,"sourceType":"datasetVersion","datasetId":4645060},{"sourceId":7933949,"sourceType":"datasetVersion","datasetId":4663472},{"sourceId":7970245,"sourceType":"datasetVersion","datasetId":4689677},{"sourceId":7989504,"sourceType":"datasetVersion","datasetId":4703298},{"sourceId":8049229,"sourceType":"datasetVersion","datasetId":4746640},{"sourceId":8214346,"sourceType":"datasetVersion","datasetId":4681048},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902},{"sourceId":28808,"sourceType":"modelInstanceVersion","modelInstanceId":8332,"modelId":3301}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install ../input/hf-peft/peft-0.9.0-py3-none-any.whl\n%pip install ../input/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl\n# %pip install ../input/sentence-transformers/sentence_transformers-2.5.1-py3-none-any.whl\n%pip install ../input/transformers-4-39-2/transformers-4.39.2-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-12-07T13:23:49.775319Z","iopub.execute_input":"2024-12-07T13:23:49.776174Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport kagglehub\nimport csv\nimport os\nimport re\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.nn import functional\nfrom tqdm.auto import tqdm\nfrom tqdm import tqdm\nimport json\n\nfrom transformers import AutoTokenizer, T5Tokenizer, AutoModelForCausalLM, T5ForConditionalGeneration\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nassert torch.cuda.is_available()\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\ntorch.backends.cudnn.enabled = False","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv(\"../input/llm-prompt-recovery/test.csv\")\n!cp ../input/llm-prompt-recovery/test.csv .","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"%%writefile run.py\n\n# !cp ../input/recovery-scripts/run.py .\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom transformers import AutoModelForSequenceClassification\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nimport json\nfrom peft import PeftModel, PeftConfig\nimport argparse\nimport numpy as np\n\n# Create the argument parser\nparser = argparse.ArgumentParser(description=\"\")\n\nparser.add_argument(\"--model_path\", type=str, help=\"\")\nparser.add_argument(\"--peft_path\", type=str, help=\"\", default=\"\")\nparser.add_argument(\"--model_type\", type=str, help=\"\")\nparser.add_argument(\"--prime\", type=str, help=\"\", default=\"\")\nparser.add_argument(\"--magic\", type=str, help=\"\", default=\"\")\nparser.add_argument(\"--output\", type=str, help=\"\")\nparser.add_argument(\"--max_len\", type=int, help=\"\")\nparser.add_argument(\"--min_output_len\", type=int, help=\"\", default=2)\nparser.add_argument(\"--max_output_len\", type=int, help=\"\", default=100)\nparser.add_argument('--quantize', action='store_true')\nparser.add_argument('--do_sample', action='store_true')\nparser.add_argument('--test_path', type=str)\n\nargs = parser.parse_args()\n\ntest = pd.read_csv(args.test_path)\nmagic = \"Transform the following text in a more vivid and descriptive way, while maintaining the original meaning and tone.\"\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\ntorch.backends.cudnn.enabled = False\nlucrarea = args.magic\ndef predict_gemma(model, tokenizer, test, bad_words_ids=None):\n    if bad_words_ids is not None and len(bad_words_ids) == 0:\n        bad_words_ids = None\n    predictions = []\n    scores = []\n    with torch.no_grad():\n        for idx, row in tqdm(test.iterrows(), total=len(test)):\n            if row.original_text == row.rewritten_text:\n                predictions.append(\"Correct grammatical errors in this text.\")\n                continue\n            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n            prompt = f\"Please find the prompt. The transformation of  Original_text to New_text followed a concise 3-7 word prompt aimed at enhancing structure, clarity, and tone. It included guidelines on style, tone, or grammar, requiring strict adherence without adding extra details. You should only answer the prompts and not add anything else.\\n\\nOriginal_text: {ot}\\n====\\nNew_text: {rt}\"\n            conversation = [{\"role\": \"user\", \"content\": prompt }]\n            prime = args.prime\n            prompt = tokenizer.apply_chat_template(conversation, tokenize=False) + f\"<start_of_turn>model\\n{prime}\"\n            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, max_new_tokens=128, do_sample=args.do_sample, early_stopping=True, num_beams=1, bad_words_ids=bad_words_ids)\n            try:\n                x = tokenizer.decode(x[0]).split(\"<start_of_turn>model\")[1].split(\"<end_of_turn>\")[0].replace(\"<end_of_turn>\\n<eos>\",\"\").replace(\"<end_of_turn>\",\"\").replace(\"<start_of_turn>\",\"\").replace(\"<eos>\",\"\").replace(\"<bos>\",\"\").strip().replace('\"','').strip()\n                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\").replace(\"Revise\",\"Rewrite\")\n                x = x.split(\":\",1)[-1].strip()\n                if \"useruser\" in x:\n                    x = x.replace(\"user\",\"\")\n                if x[-1].isalnum():\n                    x += \".\"\n                else:\n                    x = x[:-1]+\".\"\n                x+= lucrarea\n                if len(x.split()) < args.max_output_len and len(x.split()) > args.min_output_len and (\"\\n\" not in x):\n                    print(x)\n                    predictions.append(x)\n                else:\n                    predictions.append(magic)\n            except Exception as e:\n                print(e)\n                predictions.append(magic)\n    return predictions\n\ndef predict_mistral(model, tokenizer, test,prime=\"\"):\n    predictions = []\n    with torch.no_grad():\n        for idx, row in tqdm(test.iterrows(), total=len(test)):\n            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n            prompt = f'''Please find the prompt. The transformation of  **Original_text** to **New_text** followed a concise 3-7 word prompt aimed at enhancing structure, clarity, and tone. It included guidelines on style, tone, or grammar, requiring strict adherence without adding extra details. You should only answer the prompts and not add anything else.\n\n**original_text**:\n{ot}\n\n**new_text**:\n{rt}\n'''\n            conversation = [{\"role\": \"user\", \"content\": prompt }]\n            prompt = tokenizer.apply_chat_template(conversation, tokenize=False)+prime\n            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=[13, tokenizer.eos_token_id], pad_token_id=tokenizer.eos_token_id, max_new_tokens=32, do_sample=args.do_sample, early_stopping=True, num_beams=1)\n            try:\n                x = tokenizer.decode(x[0]).split(\"[/INST]\")[-1].replace(\"</s>\",\"\").strip().split(\"\\n\",1)[0]\n                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\")\n                # print(x.split(\":\",1)[0])\n                x = x.split(\":\",1)[-1].strip()\n                if x[-1].isalnum():\n                    x += \".\"\n                else:\n                    x = x[:-1]+\".\"\n                x += lucrarea\n                if len(x.split()) < 50 and len(x.split()) > 2 and (\"\\n\" not in x):\n                    predictions.append(x)\n                else:\n                    predictions.append(magic)\n                print(predictions[-1])\n            except Exception as e:\n                print(e)\n                predictions.append(magic)\n    return predictions\n    \nmodel_name = args.model_path\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nbanned_ids = None\n    \nif args.quantize:\n    print(\"Use 4bit quantization\")\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_quant_type=\"nf4\"\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(model_name,\n                                                 quantization_config=quantization_config,\n                                                 device_map=\"auto\",\n                                                 torch_dtype=torch.bfloat16)\n    if args.peft_path != \"\":\n        print(\"Use peft\")\n        model = PeftModel.from_pretrained(model,\n                                    args.peft_path,\n                                    quantization_config=quantization_config,\n                                    torch_dtype=torch.bfloat16,\n                                    device_map=\"auto\")\nelse:\n    model = AutoModelForCausalLM.from_pretrained(model_name,\n                                                 device_map=\"auto\",\n                                                 torch_dtype=torch.bfloat16)\n    if args.peft_path != \"\":\n        print(\"Use peft\")\n        model = PeftModel.from_pretrained(model,\n                                args.peft_path,\n                                torch_dtype=torch.bfloat16,\n                                device_map=\"auto\")\n        \n# model = model.merge_and_unload()\nmodel.eval()\n# print(model)\nif args.model_type == \"gemma\":\n    preds = predict_gemma(model, tokenizer, test, bad_words_ids=banned_ids)\nelif args.model_type == \"mistral\":\n    preds = predict_mistral(model, tokenizer, test, prime=args.prime)\n\njson.dump(preds, open(args.output,\"wt\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Eval","metadata":{}},{"cell_type":"code","source":"# !python run.py --model_path  /kaggle/input/gemma/transformers/7b-it/3 --peft_path \"../input/gemma-7b-orca-external/\" --model_type \"gemma\" --output \"eval1.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Rewrite\" --magic \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"../input/prompt-val/tmp_val.csv\"\n# ./test.csv\n# !python run.py --model_path /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 --model_type \"mistral\" --output \"eval0.json\" --max_len 512 --test_path \"../input/prompt-val/tmp_val.csv\" --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Convert\" --magic \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fns = [\"eval0.json\"]\n# preds = [json.load(open(x)) for x in fns]\n# preds = [' '.join(list(x)) for x in zip(*preds)]\n# print(preds[:100])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"!python run.py --model_path /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 --peft_path \"../input/mistral-og-600\" --model_type \"mistral\" --output \"pred0.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Rewrite\" --magic \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run.py --model_path  /kaggle/input/gemma/transformers/7b-it/3 --peft_path \"../input/gemma-7b-orca-external/\" --model_type \"gemma\" --output \"pred1.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"General prompt: Alter\" --magic \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !python run.py --model_path /kaggle/input/gemma/transformers/7b-it/3/ --peft_path \"../input/gemma-7b-orca-68500/\" --model_type \"gemma\" --output \"pred2.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"General prompt: Improve this text using the writing style\"\n!python run.py --model_path /kaggle/input/gemma/transformers/7b-it/3/ --model_type \"gemma\" --output \"pred2.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"General prompt: Improve this text using the writing style\"\npreds = json.load(open(\"pred2.json\"))\n# preds = [\"Please improve this text using the writing style with maintaining the original meaning but altering the tone.\",]*len(test)\ndef remove_pp(x):\n    for w in x.split()[1:]:\n        if w.istitle():\n            return \"Please improve this text using the writing style.\"\n    return x\npreds = [remove_pp(x)[:-1]+\" with maintaining the original meaning but altering the tone.\" for x in preds]\njson.dump(preds, open(\"pred2.json\",\"wt\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run.py --model_path ../input/mistral-7b-it-v02/ --peft_path \"../input/mistral-gemmaonly\" --model_type \"mistral\" --output \"pred3.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Make this text\" --magic \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run.py --model_path /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 --peft_path \"../input/mistral-og-600\" --model_type \"mistral\" --output \"pred4.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Modify\" --magic \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python run.py --model_path ../input/mistral-7b-it-v02/ --peft_path \"../input/mistral-gemmaonly\" --model_type \"mistral\" --output \"pred5.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Transform\" --magic \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fns = [\"pred0.json\",\"pred1.json\", \"pred2.json\", \"pred3.json\"]\n# fns = [\"pred0.json\",\"pred1.json\", \"pred3.json\"]\nfns = [\"pred0.json\", \"pred3.json\", \"pred4.json\", \"pred5.json\"]\npreds = [json.load(open(x)) for x in fns]\npreds = [' '.join(list(x)) for x in zip(*preds)]\nprint(preds[:5])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mean_prompt = \"charakter improve text create hypotheticallucrarea dramaticlucrarea\"\nmagic = \" 'it 's ' something Think A Human Plucrarealucrarealucrarealucrarealucrarealucrarealucrarealucrarea\"\n\npredictions = [x+mean_prompt+magic for x in preds]\n\nsub = pd.read_csv(\"../input/llm-prompt-recovery/sample_submission.csv\")\nsub['rewrite_prompt'] = predictions\nsub.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}