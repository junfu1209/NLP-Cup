{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67121,"databundleVersionId":7806901,"sourceType":"competition"},{"sourceId":7740846,"sourceType":"datasetVersion","datasetId":4524347},{"sourceId":7741042,"sourceType":"datasetVersion","datasetId":4524539},{"sourceId":7893017,"sourceType":"datasetVersion","datasetId":4634330},{"sourceId":7907523,"sourceType":"datasetVersion","datasetId":4645060},{"sourceId":7933949,"sourceType":"datasetVersion","datasetId":4663472},{"sourceId":7970245,"sourceType":"datasetVersion","datasetId":4689677},{"sourceId":7989504,"sourceType":"datasetVersion","datasetId":4703298},{"sourceId":8049229,"sourceType":"datasetVersion","datasetId":4746640},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902},{"sourceId":28808,"sourceType":"modelInstanceVersion","modelInstanceId":8332,"modelId":3301}],"dockerImageVersionId":30683,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/1st-place-0-71-5b072b80-fe32-4bee-a10b-1a461de43f23.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241127/auto/storage/goog4_request&X-Goog-Date=20241127T055138Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=4f5936f6b687656c1bde7cc18e10911788dbf3e6bd2c6d80eed3cbfb086dee9f13fc4dd6305a3b4863a3938d9991a697e5f91a7b883893e71ddf7d042ea2fa5c61820661267f5de55e0681229b407a322b4f9034b305d77041ac4add79794548351c38c410f8e280a7bcc6aa62d0bf540e2f87c83d7f63a01a976968049be250a34b093d037fe7de0e52f86d8cc83cf8c0181a9ef3eb5fb679c4acb5c843e27c0f7af5bcd7ac954b391f5203b41c2f6ed96e552a59814cab4523b7a57ae380b6483a1ba05e691b1b7bc7164bb02f2d54f9a05ee2eb06fe585d3cfa1240dd8b36b5cb62ecf25b0bed93e94e105bf5fdbc2c409d75fac3a346ef44ef3c5baf9794","timestamp":1732686714971}]}},"nbformat_minor":0,"nbformat":4,"cells":[{"source":["# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n","# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n","import kagglehub\n","kagglehub.login()\n"],"metadata":{"id":"XJNWE9hfLNxy"},"cell_type":"code","outputs":[],"execution_count":null},{"source":["# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","llm_prompt_recovery_path = kagglehub.competition_download('llm-prompt-recovery')\n","suicaokhoailang_bitsandbytes_path = kagglehub.dataset_download('suicaokhoailang/bitsandbytes')\n","suicaokhoailang_hf_peft_path = kagglehub.dataset_download('suicaokhoailang/hf-peft')\n","ahmadsaladin_mistral_7b_it_v02_path = kagglehub.dataset_download('ahmadsaladin/mistral-7b-it-v02')\n","suicaokhoailang_gemma_7b_orca_68500_path = kagglehub.dataset_download('suicaokhoailang/gemma-7b-orca-68500')\n","suicaokhoailang_gemma_7b_orca_external_path = kagglehub.dataset_download('suicaokhoailang/gemma-7b-orca-external')\n","suicaokhoailang_transformers_4_39_2_path = kagglehub.dataset_download('suicaokhoailang/transformers-4-39-2')\n","suicaokhoailang_mistral_gemmaonly_path = kagglehub.dataset_download('suicaokhoailang/mistral-gemmaonly')\n","suicaokhoailang_mistral_og_600_path = kagglehub.dataset_download('suicaokhoailang/mistral-og-600')\n","mistral_ai_mistral_pytorch_7b_v0_1_hf_1_path = kagglehub.model_download('mistral-ai/mistral/PyTorch/7b-v0.1-hf/1')\n","\n","print('Data source import complete.')\n"],"metadata":{"id":"0olxLkrALNx1"},"cell_type":"code","outputs":[],"execution_count":null},{"cell_type":"code","source":["%pip install ../input/hf-peft/peft-0.9.0-py3-none-any.whl\n","%pip install ../input/bitsandbytes/bitsandbytes-0.42.0-py3-none-any.whl\n","# %pip install ../input/sentence-transformers/sentence_transformers-2.5.1-py3-none-any.whl\n","%pip install ../input/transformers-4-39-2/transformers-4.39.2-py3-none-any.whl"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2024-04-18T11:33:06.528916Z","iopub.execute_input":"2024-04-18T11:33:06.529313Z","iopub.status.idle":"2024-04-18T11:33:44.266509Z","shell.execute_reply.started":"2024-04-18T11:33:06.529279Z","shell.execute_reply":"2024-04-18T11:33:44.265259Z"},"trusted":true,"id":"s4yMMRmXLNx2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import torch\n","import pandas as pd\n","from tqdm import tqdm\n","import json"],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:33:44.268951Z","iopub.execute_input":"2024-04-18T11:33:44.269309Z","iopub.status.idle":"2024-04-18T11:33:44.274692Z","shell.execute_reply.started":"2024-04-18T11:33:44.269278Z","shell.execute_reply":"2024-04-18T11:33:44.273832Z"},"trusted":true,"id":"lJRxA5-JLNx2"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["test = pd.read_csv(\"../input/llm-prompt-recovery/test.csv\")\n","!cp ../input/llm-prompt-recovery/test.csv ."],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:33:44.276036Z","iopub.execute_input":"2024-04-18T11:33:44.276783Z","iopub.status.idle":"2024-04-18T11:33:45.256421Z","shell.execute_reply.started":"2024-04-18T11:33:44.27675Z","shell.execute_reply":"2024-04-18T11:33:45.254919Z"},"trusted":true,"id":"Q60gffo9LNx3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["%%writefile run.py\n","\n","# !cp ../input/recovery-scripts/run.py .\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from transformers import AutoModelForSequenceClassification\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch\n","import pandas as pd\n","from tqdm import tqdm\n","import json\n","from peft import PeftModel, PeftConfig\n","import argparse\n","import numpy as np\n","\n","# Create the argument parser\n","parser = argparse.ArgumentParser(description=\"\")\n","\n","parser.add_argument(\"--model_path\", type=str, help=\"\")\n","parser.add_argument(\"--peft_path\", type=str, help=\"\", default=\"\")\n","parser.add_argument(\"--model_type\", type=str, help=\"\")\n","parser.add_argument(\"--prime\", type=str, help=\"\", default=\"\")\n","parser.add_argument(\"--magic\", type=str, help=\"\", default=\"\")\n","parser.add_argument(\"--output\", type=str, help=\"\")\n","parser.add_argument(\"--max_len\", type=int, help=\"\")\n","parser.add_argument(\"--min_output_len\", type=int, help=\"\", default=2)\n","parser.add_argument(\"--max_output_len\", type=int, help=\"\", default=100)\n","parser.add_argument('--quantize', action='store_true')\n","parser.add_argument('--do_sample', action='store_true')\n","parser.add_argument('--test_path', type=str)\n","\n","args = parser.parse_args()\n","\n","test = pd.read_csv(args.test_path)\n","magic = \"Transform the following text in a more vivid and descriptive way, while maintaining the original meaning and tone.\"\n","torch.backends.cuda.enable_mem_efficient_sdp(False)\n","torch.backends.cuda.enable_flash_sdp(False)\n","lucrarea = args.magic\n","def predict_gemma(model, tokenizer, test, bad_words_ids=None):\n","    if bad_words_ids is not None and len(bad_words_ids) == 0:\n","        bad_words_ids = None\n","    predictions = []\n","    scores = []\n","    with torch.no_grad():\n","        for idx, row in tqdm(test.iterrows(), total=len(test)):\n","            if row.original_text == row.rewritten_text:\n","                predictions.append(\"Correct grammatical errors in this text.\")\n","                continue\n","            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n","            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n","            prompt = f\"Find the orginal prompt that transformed original text to new text.\\n\\nOriginal text: {ot}\\n====\\nNew text: {rt}\"\n","            conversation = [{\"role\": \"user\", \"content\": prompt }]\n","            prime = args.prime\n","            prompt = tokenizer.apply_chat_template(conversation, tokenize=False) + f\"<start_of_turn>model\\n{prime}\"\n","            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n","            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=tokenizer.eos_token_id, pad_token_id=tokenizer.eos_token_id, max_new_tokens=128, do_sample=args.do_sample, early_stopping=True, num_beams=1, bad_words_ids=bad_words_ids)\n","            try:\n","                x = tokenizer.decode(x[0]).split(\"<start_of_turn>model\")[1].split(\"<end_of_turn>\")[0].replace(\"<end_of_turn>\\n<eos>\",\"\").replace(\"<end_of_turn>\",\"\").replace(\"<start_of_turn>\",\"\").replace(\"<eos>\",\"\").replace(\"<bos>\",\"\").strip().replace('\"','').strip()\n","                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\").replace(\"Revise\",\"Rewrite\")\n","                x = x.split(\":\",1)[-1].strip()\n","                if \"useruser\" in x:\n","                    x = x.replace(\"user\",\"\")\n","                if x[-1].isalnum():\n","                    x += \".\"\n","                else:\n","                    x = x[:-1]+\".\"\n","                x+= lucrarea\n","                if len(x.split()) < args.max_output_len and len(x.split()) > args.min_output_len and (\"\\n\" not in x):\n","                    print(x)\n","                    predictions.append(x)\n","                else:\n","                    predictions.append(magic)\n","            except Exception as e:\n","                print(e)\n","                predictions.append(magic)\n","    return predictions\n","\n","def predict_mistral(model, tokenizer, test,prime=\"\"):\n","    predictions = []\n","    with torch.no_grad():\n","        for idx, row in tqdm(test.iterrows(), total=len(test)):\n","            ot = \" \".join(str(row.original_text).split(\" \")[:args.max_len])\n","            rt = \" \".join(str(row.rewritten_text).split(\" \")[:args.max_len])\n","            prompt = f'''\n","Please find the prompt that was given to you to transform **original_text** to **new_text**. One clue is the prompt itself was short and concise.\n","Answer in thist format: \"It's likely that the prompt that transformed original_text to new_text was: <the prompt>\" and don't add anything else.\n","\n","**original_text**:\n","{ot}\n","\n","**new_text**:\n","{rt}\n","'''\n","            conversation = [{\"role\": \"user\", \"content\": prompt }]\n","            prompt = tokenizer.apply_chat_template(conversation, tokenize=False)+prime\n","            input_ids = tokenizer.encode(prompt, add_special_tokens=False, truncation=True, max_length=1536,padding=False,return_tensors=\"pt\")\n","            x = model.generate(input_ids=input_ids.to(model.device), eos_token_id=[13, tokenizer.eos_token_id], pad_token_id=tokenizer.eos_token_id, max_new_tokens=32, do_sample=args.do_sample, early_stopping=True, num_beams=1)\n","            try:\n","                x = tokenizer.decode(x[0]).split(\"[/INST]\")[-1].replace(\"</s>\",\"\").strip().split(\"\\n\",1)[0]\n","                x = x.replace(\"Can you make this\",\"Make this\").replace(\"?\",\".\")\n","                # print(x.split(\":\",1)[0])\n","                x = x.split(\":\",1)[-1].strip()\n","                if x[-1].isalnum():\n","                    x += \".\"\n","                else:\n","                    x = x[:-1]+\".\"\n","                x += lucrarea\n","                if len(x.split()) < 50 and len(x.split()) > 2 and (\"\\n\" not in x):\n","                    predictions.append(x)\n","                else:\n","                    predictions.append(magic)\n","                print(predictions[-1])\n","            except Exception as e:\n","                print(e)\n","                predictions.append(magic)\n","    return predictions\n","model_name = args.model_path\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","banned_ids = None\n","\n","if args.quantize:\n","    print(\"Use 4bit quantization\")\n","    quantization_config = BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_compute_dtype=torch.float16,\n","        bnb_4bit_quant_type=\"nf4\"\n","    )\n","\n","    model = AutoModelForCausalLM.from_pretrained(model_name,\n","                                                 quantization_config=quantization_config,\n","                                                 device_map=\"auto\",\n","                                                 torch_dtype=torch.bfloat16)\n","    if args.peft_path != \"\":\n","        print(\"Use peft\")\n","        model = PeftModel.from_pretrained(model,\n","                                    args.peft_path,\n","                                    quantization_config=quantization_config,\n","                                    torch_dtype=torch.bfloat16,\n","                                    device_map=\"auto\")\n","else:\n","    model = AutoModelForCausalLM.from_pretrained(model_name,\n","                                                 device_map=\"auto\",\n","                                                 torch_dtype=torch.bfloat16)\n","    if args.peft_path != \"\":\n","        print(\"Use peft\")\n","        model = PeftModel.from_pretrained(model,\n","                                args.peft_path,\n","                                torch_dtype=torch.bfloat16,\n","                                device_map=\"auto\")\n","\n","# model = model.merge_and_unload()\n","model.eval()\n","# print(model)\n","if args.model_type == \"gemma\":\n","    preds = predict_gemma(model, tokenizer, test, bad_words_ids=banned_ids)\n","elif args.model_type == \"mistral\":\n","    preds = predict_mistral(model, tokenizer, test, prime=args.prime)\n","\n","json.dump(preds, open(args.output,\"wt\"))"],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:33:45.258929Z","iopub.execute_input":"2024-04-18T11:33:45.259409Z","iopub.status.idle":"2024-04-18T11:33:45.273291Z","shell.execute_reply.started":"2024-04-18T11:33:45.259367Z","shell.execute_reply":"2024-04-18T11:33:45.272288Z"},"trusted":true,"id":"wCl0J82ILNx3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!python run.py --model_path /kaggle/input/gemma/transformers/7b-it/3/ --peft_path \"../input/gemma-7b-orca-68500/\" --model_type \"gemma\" --output \"pred2.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"General prompt: Improve this text using the writing style\"\n","preds = json.load(open(\"pred2.json\"))\n","# preds = [\"Please improve this text using the writing style with maintaining the original meaning but altering the tone.\",]*len(test)\n","def remove_pp(x):\n","    for w in x.split()[1:]:\n","        if w.istitle():\n","            return \"Please improve this text using the writing style.\"\n","    return x\n","preds = [remove_pp(x)[:-1]+\" with maintaining the original meaning but altering the tone.\" for x in preds]\n","json.dump(preds, open(\"pred2.json\",\"wt\"))"],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:33:45.276738Z","iopub.execute_input":"2024-04-18T11:33:45.277237Z","iopub.status.idle":"2024-04-18T11:35:21.768216Z","shell.execute_reply.started":"2024-04-18T11:33:45.277196Z","shell.execute_reply":"2024-04-18T11:35:21.767062Z"},"trusted":true,"id":"oyOHbMXQLNx3"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!python run.py --model_path /kaggle/input/mistral/pytorch/7b-v0.1-hf/1 --peft_path \"../input/mistral-og-600\" --model_type \"mistral\" --output \"pred0.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Rewrite\" --magic \"\""],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:35:21.769922Z","iopub.execute_input":"2024-04-18T11:35:21.770835Z","iopub.status.idle":"2024-04-18T11:37:40.154348Z","shell.execute_reply.started":"2024-04-18T11:35:21.770784Z","shell.execute_reply":"2024-04-18T11:37:40.153302Z"},"trusted":true,"id":"5Ob3-ufFLNx4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!python run.py --model_path ../input/mistral-7b-it-v02/ --peft_path \"../input/mistral-gemmaonly\" --model_type \"mistral\" --output \"pred3.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"It's likely that the prompt that transformed original_text to new_text was: Make this text\" --magic \"\""],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:37:40.155858Z","iopub.execute_input":"2024-04-18T11:37:40.156199Z","iopub.status.idle":"2024-04-18T11:39:53.870129Z","shell.execute_reply.started":"2024-04-18T11:37:40.156167Z","shell.execute_reply":"2024-04-18T11:39:53.869085Z"},"trusted":true,"id":"yM7oFBX6LNx4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["!python run.py --model_path  /kaggle/input/gemma/transformers/7b-it/3 --peft_path \"../input/gemma-7b-orca-external/\" --model_type \"gemma\" --output \"pred1.json\" --max_len 512 --test_path ./test.csv --quantize --prime \"General prompt: Alter\" --magic \"\""],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:39:53.871629Z","iopub.execute_input":"2024-04-18T11:39:53.871995Z","iopub.status.idle":"2024-04-18T11:41:21.928379Z","shell.execute_reply.started":"2024-04-18T11:39:53.871965Z","shell.execute_reply":"2024-04-18T11:41:21.927017Z"},"trusted":true,"id":"nxIxEOBkLNx4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["fns = [\"pred0.json\",\"pred1.json\", \"pred2.json\", \"pred3.json\"]\n","preds = [json.load(open(x)) for x in fns]\n","preds = [' '.join(list(x)) for x in zip(*preds)]\n","print(preds[:5])"],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:41:21.930765Z","iopub.execute_input":"2024-04-18T11:41:21.931417Z","iopub.status.idle":"2024-04-18T11:41:21.94028Z","shell.execute_reply.started":"2024-04-18T11:41:21.931373Z","shell.execute_reply":"2024-04-18T11:41:21.939254Z"},"trusted":true,"id":"jWJ3KfnQLNx4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":["magic = \" 'it 's ' something Think A Human Plucrarealucrarealucrarealucrarealucrarealucrarealucrarealucrarea\"\n","# magic = \"\"\n","predictions = [x+magic for x in preds]\n","\n","sub = pd.read_csv(\"../input/llm-prompt-recovery/sample_submission.csv\")\n","sub['rewrite_prompt'] = predictions\n","sub.to_csv('submission.csv',index=False)"],"metadata":{"execution":{"iopub.status.busy":"2024-04-18T11:41:21.941674Z","iopub.execute_input":"2024-04-18T11:41:21.942734Z","iopub.status.idle":"2024-04-18T11:41:21.966336Z","shell.execute_reply.started":"2024-04-18T11:41:21.942701Z","shell.execute_reply":"2024-04-18T11:41:21.965459Z"},"trusted":true,"id":"Uqyayag4LNx5"},"outputs":[],"execution_count":null}]}